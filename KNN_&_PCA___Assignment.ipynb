{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Assignment Code: DA-AG-016\n",
        "# KNN & PCA | Assignment"
      ],
      "metadata": {
        "id": "U9nOwEw5trlS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1: What is K-Nearest Neighbors (KNN) and how does it work in both\n",
        "classification and regression problems?**"
      ],
      "metadata": {
        "id": "_i7rpV4UtzW-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:-\n",
        "\n",
        "**K-Nearest Neighbors (KNN)?**\n",
        "\n",
        "   - K-Nearest Neighbors (KNN) is a supervised machine learning algorithm used for both classification and regression tasks.\n",
        "   - It is a non-parametric (doesn’t assume any data distribution) and instance-based (lazy learner) algorithm.\n",
        "\n",
        " **KNN does Work in both classification and regression problems**\n",
        "\n",
        "  1. **Choose K **(the number of neighbors, e.g., K = 3 or 5).\n",
        "\n",
        "  2. **Calculate distance** between the new data point and all training points (commonly Euclidean distance, but Manhattan, Minkowski, or cosine similarity can also be used).\n",
        "\n",
        "  3. **Find the K nearest neighbors** (training points with the smallest distances).\n",
        "\n",
        "  4. **Make prediction:**\n",
        "\n",
        "   - **Classification** → assign the class that is most frequent among the K neighbors (majority voting).\n",
        "\n",
        "   - **Regression** → take the average (or weighted average, where closer neighbors have more weight) of the K neighbors’ values."
      ],
      "metadata": {
        "id": "KanSITZqt9QT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2: What is the Curse of Dimensionality and how does it affect KNN performance?**"
      ],
      "metadata": {
        "id": "dJIs8UB5vz7X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:-\n",
        "\n",
        " **The Curse of Dimensionality**\n",
        "\n",
        "   - The curse of dimensionality refers to the various problems that arise when working with high-dimensional data (data with a large number of features).\n",
        "\n",
        "**How it Affects KNN Performance**\n",
        "\n",
        "**1. Distance Becomes Less Informative**\n",
        "\n",
        "  - In high dimensions, the distance between the nearest neighbor and the farthest neighbor tends to become almost the same.\n",
        "\n",
        "  - This makes it hard for KNN to distinguish between \"close\" and \"far\" neighbors.\n",
        "\n",
        " **2. Increased Computational Cost**\n",
        "\n",
        "  - More dimensions → more distance calculations → slower performance.\n",
        "\n",
        " **3. Risk of Overfitting**\n",
        "\n",
        "  - High-dimensional data is sparse, so KNN may overfit to noise instead of finding meaningful neighbors.\n",
        "\n",
        " **4. Feature Irrelevance**\n",
        "\n",
        "  - If many features are irrelevant, they can dilute the impact of useful features when computing distance."
      ],
      "metadata": {
        "id": "2_Q8MmztwdYy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3: What is Principal Component Analysis (PCA)? How is it different from feature selection?**"
      ],
      "metadata": {
        "id": "8iNA8xg9xmmt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:-\n",
        "\n",
        "**Principal Component Analysis (PCA)**\n",
        "\n",
        "  - Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms high-dimensional data into a smaller set of new features (called principal components) while retaining as much variance (information) as possible.\n",
        "\n",
        "  **How PCA is Different from Feature Selection**\n",
        "\n",
        "\n",
        "| Aspect               | **PCA (Dimensionality Reduction)**                                                             | **Feature Selection**                                                          |\n",
        "| -------------------- | ---------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------ |\n",
        "| **Definition**       | Creates new features (principal components) that are linear combinations of original features. | Selects a subset of the original features, removing irrelevant/redundant ones. |\n",
        "| **Approach**         | **Transforms** features into a new space.                                                      | **Keeps** the most important original features.                                |\n",
        "| **Interpretability** | Components are harder to interpret (since they are mixes of original features).                | Easy to interpret (you know exactly which features are used).                  |\n",
        "| **Goal**             | Reduce dimensionality while preserving maximum variance.                                       | Reduce dimensionality by discarding irrelevant or redundant features.          |\n",
        "| **Type**             | **Feature extraction** technique.                                                              | **Feature selection** technique.                                               |\n"
      ],
      "metadata": {
        "id": "W68Zgcjcxtls"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4: What are eigenvalues and eigenvectors in PCA, and why are they\n",
        "important?**"
      ],
      "metadata": {
        "id": "C7tIGBN1ySZ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:-\n",
        "\n",
        "**Eigenvalues and Eigenvectors (in PCA context)**\n",
        "\n",
        "**1. Eigenvectors**\n",
        "\n",
        "  - Directions (axes) in the feature space along which the data varies the most.\n",
        "\n",
        "  - In PCA, these eigenvectors are the principal components (new axes).\n",
        "\n",
        "  - They define the orientation of the new feature space.\n",
        "\n",
        "**2. Eigenvalues**\n",
        "\n",
        "  - Numbers that tell us how much variance (information) is captured by each eigenvector (principal component).\n",
        "\n",
        "  - A larger eigenvalue means that direction (eigenvector) explains more variance in the data.\n",
        "\n",
        "  **Why They Are Important in PCA**\n",
        "\n",
        "  Imagine shining a flashlight on a 3D object to create a shadow on the wall (a 2D projection):\n",
        "\n",
        "  - The **eigenvectors** are the directions of the flashlight beam (how we project).\n",
        "\n",
        "  - The **eigenvalues** tell us how much of the object’s shape is preserved in each projection."
      ],
      "metadata": {
        "id": "05pBzwrByZK-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5: How do KNN and PCA complement each other when applied in a single pipeline?**\n"
      ],
      "metadata": {
        "id": "CgJFS8o0zXKN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:-\n",
        "\n",
        "** How PCA Helps KNN**\n",
        "\n",
        " PCA can be applied before KNN to improve performance:\n",
        "\n",
        " 1.** Dimensionality Reduction**\n",
        "\n",
        " - PCA reduces the number of features while keeping most of the variance.\n",
        "\n",
        " - This makes distance calculations in KNN more reliable.\n",
        "\n",
        " **2. Noise Removal**\n",
        "\n",
        " - PCA ignores components with very small eigenvalues (low variance = likely noise).\n",
        "\n",
        " - Cleaner data → KNN neighbors become more meaningful.\n",
        "\n",
        "**3. Faster Computation**\n",
        "\n",
        " - Fewer dimensions = fewer distance calculations.\n",
        "\n",
        " - KNN becomes more scalable.\n",
        "\n",
        "**4. Better Generalization**\n",
        "\n",
        " - By removing irrelevant/weak components, PCA reduces overfitting risk in KNN."
      ],
      "metadata": {
        "id": "WCRWkhhhzlLn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6Ov7Z3itqIT"
      },
      "outputs": [],
      "source": [
        "# Dataset:\n",
        "from sklearn.datasets.load_wine()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6: Train a KNN Classifier on the Wine dataset with and without feature scaling. Compare model accuracy in both cases.**\n"
      ],
      "metadata": {
        "id": "RC_rGrtW1oG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ans:- KNN on Wine dataset with and without scaling\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# ---------------- Without Scaling ----------------\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred_no_scaling = knn.predict(X_test)\n",
        "acc_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# ---------------- With Scaling ----------------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Print results\n",
        "print(\"KNN Accuracy without Scaling:\", acc_no_scaling)\n",
        "print(\"KNN Accuracy with Scaling   :\", acc_scaled)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9RXoEON1nh2",
        "outputId": "90b20e31-9e05-4ce1-ecc0-092ca18249ea"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Accuracy without Scaling: 0.7222222222222222\n",
            "KNN Accuracy with Scaling   : 0.9444444444444444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7: Train a PCA model on the Wine dataset and print the explained variance ratio of each principal component.**\n"
      ],
      "metadata": {
        "id": "tKlGNLxH2g7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ans:- PCA on Wine dataset - Explained Variance Ratio\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Step 1: Standardize features (important for PCA)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Step 2: Apply PCA (keep all components for analysis)\n",
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Step 3: Print explained variance ratio\n",
        "print(\"Explained Variance Ratio of each Principal Component:\")\n",
        "for i, ratio in enumerate(pca.explained_variance_ratio_):\n",
        "    print(f\"PC{i+1}: {ratio:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52YG_kXJ2PWP",
        "outputId": "c7d3d6f9-876d-45f4-f72b-d161a80c845c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained Variance Ratio of each Principal Component:\n",
            "PC1: 0.3620\n",
            "PC2: 0.1921\n",
            "PC3: 0.1112\n",
            "PC4: 0.0707\n",
            "PC5: 0.0656\n",
            "PC6: 0.0494\n",
            "PC7: 0.0424\n",
            "PC8: 0.0268\n",
            "PC9: 0.0222\n",
            "PC10: 0.0193\n",
            "PC11: 0.0174\n",
            "PC12: 0.0130\n",
            "PC13: 0.0080\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2 components). Compare the accuracy with the original dataset.**"
      ],
      "metadata": {
        "id": "0J0_66TS25QS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ans:- KNN on PCA-transformed Wine dataset vs Original dataset\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Split into train-test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# ---------------- Original Dataset with Scaling ----------------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_original = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_original.fit(X_train_scaled, y_train)\n",
        "y_pred_original = knn_original.predict(X_test_scaled)\n",
        "acc_original = accuracy_score(y_test, y_pred_original)\n",
        "\n",
        "# ---------------- PCA-transformed Dataset (Top 2 PCs) ----------------\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "acc_pca = accuracy_score(y_test, y_pred_pca)\n",
        "\n",
        "# Print results\n",
        "print(\"KNN Accuracy on Original Dataset:\", acc_original)\n",
        "print(\"KNN Accuracy on PCA (2 components):\", acc_pca)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vulgFxUF29TC",
        "outputId": "bcc6ae1c-7dc3-4bae-cfc4-79f7d38f3dc5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Accuracy on Original Dataset: 0.9444444444444444\n",
            "KNN Accuracy on PCA (2 components): 0.9444444444444444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9: Train a KNN Classifier with different distance metrics (euclidean, manhattan) on the scaled Wine dataset and compare the results.\n",
        "(Include your Python code and output in the code box below.)**"
      ],
      "metadata": {
        "id": "iwlBGHhH3WC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ans:- KNN with different distance metrics on Wine dataset\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# ---------------- KNN with Euclidean Distance ----------------\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "knn_euclidean.fit(X_train_scaled, y_train)\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test_scaled)\n",
        "acc_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "\n",
        "# ---------------- KNN with Manhattan Distance ----------------\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
        "knn_manhattan.fit(X_train_scaled, y_train)\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test_scaled)\n",
        "acc_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "\n",
        "# Print results\n",
        "print(\"KNN Accuracy with Euclidean Distance:\", acc_euclidean)\n",
        "print(\"KNN Accuracy with Manhattan Distance:\", acc_manhattan)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Z8Bm7om29Pf",
        "outputId": "46417608-f523-49c4-e690-a2e1c5d5f9d3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Accuracy with Euclidean Distance: 0.9444444444444444\n",
            "KNN Accuracy with Manhattan Distance: 0.9814814814814815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10: You are working with a high-dimensional gene expression dataset to classify patients with different types of cancer.**\n",
        "\n",
        "**Due to the large number of features and a small number of samples, traditional models overfit.**\n",
        "\n",
        "Explain how you would:\n",
        "\n",
        "  - Use PCA to reduce dimensionality\n",
        "  -  Decide how many components to keep\n",
        "  - Use KNN for classification post-dimensionality reduction\n",
        "  -  Evaluate the model\n",
        "  -  Justify this pipeline to your stakeholders as a robust solution for real-world biomedical data\n"
      ],
      "metadata": {
        "id": "c8RyTAo83zK8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:-\n",
        "\n",
        "**Step 1: Use PCA to Reduce Dimensionality**\n",
        "\n",
        "  - Gene expression datasets often have thousands of features (genes) but few samples (patients).\n",
        "\n",
        " - PCA helps by projecting the data into fewer principal components, capturing most of the variance while removing noise.\n",
        "\n",
        " **Step 2: Decide How Many Components to Keep**\n",
        "\n",
        " - Use the explained variance ratio (scree plot or cumulative variance).\n",
        "\n",
        " - Choose the smallest number of components that explains, say, 90–95% of the variance.\n",
        "\n",
        " - This balances information retention vs. dimensionality reduction.\n",
        "\n",
        " **Step 3: Apply KNN Post-PCA**\n",
        "\n",
        " - Run KNN on PCA-transformed data.\n",
        "\n",
        " - Tune K (neighbors) using cross-validation.\n",
        "\n",
        " - Since PCA removes noise, distances between samples are more meaningful.\n",
        "\n",
        " **Step 4: Evaluate the Model**\n",
        "\n",
        " - Use Stratified k-Fold Cross-Validation (important because dataset is small and imbalanced).\n",
        "\n",
        " - Metrics: Accuracy, F1-score, Confusion Matrix (since cancer types may be imbalanced).\n",
        "\n",
        " **Step 5: Justification to Stakeholders**\n",
        "\n",
        " - PCA prevents overfitting by reducing noise and redundant features.\n",
        "\n",
        " - KNN is simple, interpretable, and distance-based, which makes sense for gene similarity.\n",
        "\n",
        " - Pipeline is robust and generalizable, suitable for biomedical research where sample sizes are small.\n",
        "\n",
        " - Can identify patterns in gene expression without requiring complex black-box models."
      ],
      "metadata": {
        "id": "IRZIc5-74joL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Python Code (Example with Wine dataset as proxy for gene expression)**"
      ],
      "metadata": {
        "id": "ZGmeAWdm5Ob9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PCA + KNN pipeline for high-dimensional biomedical data (example with Wine dataset)\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset (using Wine dataset as an example proxy)\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA (retain 95% variance)\n",
        "pca = PCA(n_components=0.95)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "print(\"Original features:\", X.shape[1])\n",
        "print(\"Reduced features (PCA):\", X_pca.shape[1])\n",
        "\n",
        "# Train KNN classifier on PCA-transformed data\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# Evaluate with Stratified 5-Fold Cross-Validation\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scores = cross_val_score(knn, X_pca, y, cv=cv, scoring='accuracy')\n",
        "\n",
        "print(\"\\nCross-validation accuracies:\", scores)\n",
        "print(\"Mean Accuracy:\", np.mean(scores))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nm3AGFV82ycH",
        "outputId": "de4e2da2-8802-4aeb-d2fd-ee321461dd93"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original features: 13\n",
            "Reduced features (PCA): 10\n",
            "\n",
            "Cross-validation accuracies: [0.97222222 0.97222222 0.97222222 0.94285714 0.97142857]\n",
            "Mean Accuracy: 0.9661904761904762\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "27kgsZur5X_L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}